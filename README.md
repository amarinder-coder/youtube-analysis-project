# youtube-analysis-project

complete architecture to follow:
![image](https://user-images.githubusercontent.com/66850958/225360550-0e50dddd-9797-41f4-8907-13a329cf7940.png)


step1: configure aws using aws cli in local
![image](https://user-images.githubusercontent.com/66850958/225161817-7959e5f4-ad16-47b8-beb8-67566f1f3157.png)


step2: create s3 buckets and upload files to s3 bucket using aws cli.
![image](https://user-images.githubusercontent.com/66850958/225358142-4e544113-0352-46fe-a8dd-921f4ea1c177.png)

step3: create glue crawler to crawl data from these files with iam role to access s3 buckets.
![image](https://user-images.githubusercontent.com/66850958/225358652-2e5223e9-3d38-419c-a4df-379b30aa74cb.png)

step4: Create lambda function to convert referenced data json into normalized parquet to be read by athena. the code would be available in lambda_function.py file.
Added awsdarawrangler module as a layer to support execution and attached iam role with s3 and glue policies.
![image](https://user-images.githubusercontent.com/66850958/225359428-d936d7fa-38e3-44ce-9db5-00c0761d218c.png)

step5: create glue crawler for csv raw data.
![image](https://user-images.githubusercontent.com/66850958/225361481-562c3fc2-a9f7-470a-bfb0-f6a539f34130.png)

step6: Writing sql query in athena to join csv and cleaned parquet files based on region key to easily filter out records.
![image](https://user-images.githubusercontent.com/66850958/225361047-14ff5e2c-e588-4ebf-b235-17938e201deb.png)


step7: create etl job to automate process to fetch all csv raw files of all regions, convert to parquet and store in cleaned s3 bucket. 
The glue script is etl_job.py 

#Here we are only pushing gb us and ca region folder files for simplicity. 
![image](https://user-images.githubusercontent.com/66850958/225362453-65debaf4-62c4-4f8f-8efc-c4c1bb6e89b5.png)

Successful output of above job in s3 with partitioned based on regions.
![image](https://user-images.githubusercontent.com/66850958/225363351-a71c0804-e2bd-4945-b414-a5fb7e0042e5.png)


step8:create glue crawler based on these cleaned parquet files.
![image](https://user-images.githubusercontent.com/66850958/225364800-d842e403-4c62-488e-a4c1-47065895f549.png)


step9: create lambda trigger to run whenever new file is added in raw_referenced_data folder to be converted to cleaned parquet.
![image](https://user-images.githubusercontent.com/66850958/225365814-798435e6-12f4-4e15-891b-0b9b5450db29.png)

delete files to test the above configuration and reupload them.
![image](https://user-images.githubusercontent.com/66850958/225366729-25347cfb-81fd-4f05-a32e-c60d98da922a.png)


After reuploading, the cleaned s3 bucket got updated by the new files generated by lambda trigger.
![image](https://user-images.githubusercontent.com/66850958/225367029-6c4a37f6-54d6-4127-aa6a-f7d9dc960e79.png)

joining query from cleaned json and csv data.
![image](https://user-images.githubusercontent.com/66850958/225369569-0e8f35f9-1217-41bc-b08e-fe79c78d2f4e.png)

step10:create etl job to perform above query joining in glue. create a new s3 analytics bucket to store final output.
![image](https://user-images.githubusercontent.com/66850958/225372652-8b86885b-13f2-4a65-9fba-823e503775b5.png)


successful output in analytics bucket as partitioned on region and category_id.
![image](https://user-images.githubusercontent.com/66850958/225372926-26274fe9-d769-469d-afc2-8da54e8393ce.png)

output in athena.
![image](https://user-images.githubusercontent.com/66850958/225373262-4384dfe2-999d-4df2-99ad-d704ecbe32de.png)

step10: create quicksight account and build a visual report based on analytic bucket data.
![image](https://user-images.githubusercontent.com/66850958/225374336-eb7be635-e2c1-4260-a5be-643d03958a86.png)

step11:create a new dataset and connect to athena awsdatacatalog.
![image](https://user-images.githubusercontent.com/66850958/225377248-040a3eeb-7958-4bc6-82bb-917aeca9875a.png)


Make a dashboard and export it as per your need.
![image](https://user-images.githubusercontent.com/66850958/225379132-a8cf9a97-8171-4223-9e8d-f21d4841051f.png)



